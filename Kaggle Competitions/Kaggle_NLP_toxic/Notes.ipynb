{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- NB-SVM\n",
    "- Check how scoring works "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There are two major components we need to consider when it comes to Multi-label Classifiers ([source](https://users.ics.aalto.fi/jesse/talks/Multilabel-Part01.pdf)):\n",
    "1. Methods for Multi-label Classfication:\n",
    "    - Problem Transformation\n",
    "    - Algorithm Adaptation\n",
    "2. Multi-label Evaluation\n",
    "\n",
    "## 1. Methods for Multi-label Classification\n",
    "### a) Problem Transformation:\n",
    "- Transforms the multi-label problem into single-label problem(s)\n",
    "- Use any off-the-shelf single-label classifier to suit requirements\n",
    "- i.e., Adapt your data to the algorithm\n",
    "\n",
    "At training time:\n",
    "1. Transform the multi-label training data to single-label data\n",
    "2. Learn from the single-label transformed data\n",
    "\n",
    "At testing time:\n",
    "1. Make single-label predictions\n",
    "2. Translate these into multi-label predictions\n",
    "\n",
    "Examples:\n",
    "1. Binary Relevance: $L$ binary problems (one vs. all):\n",
    "2. Label Powerset: one multi-class problem of $2L$ class-values\n",
    "3. Pairwise: $L(L−1)/2$ binary problems (all vs. all)\n",
    "4. Copy-Weight: one multi-class problem of $L$ class values\n",
    "5. [Classifier Chains](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)\n",
    "\n",
    "### Comment:\n",
    "- Some algorithms (like BR) do not model **label dependency!**\n",
    "- The label ordering issue: Label ordering matters in algorithms such as Classifier chains. (There are some DL architectures that try to alleviate this problem e.g. [see](http://proceedings.mlr.press/v101/yang19b/yang19b.pdf))\n",
    "\n",
    "### b) Algorithm Adaptation Methods:\n",
    "- Adapt a single-label algorithm to produce multi-label outputs\n",
    "- Benefit from specific classifier advantages (e.g., efficiency)\n",
    "- i.e., Adapt your algorithm to the data\n",
    "\n",
    "Examples: Adapt \n",
    "1. k-Nearest Neighbours\n",
    "2. Decision Trees\n",
    "3. Neural Networks\n",
    "4. Support Vector Machines\n",
    "5. Ensemble methods (Random Forest, XGboost, etc)\n",
    "\n",
    "to learn from multi-label data and make multi-label predictions.\n",
    "\n",
    "## 2. Multi-label Evaluation \n",
    "[source](https://www.researchgate.net/profile/Mohammad_Sorower/publication/266888594_A_Literature_Survey_on_Algorithms_for_Multi-label_Learning/links/58d1864392851cf4f8f4b72a/A-Literature-Survey-on-Algorithms-for-Multi-label-Learning.pdf)\n",
    "\n",
    "1. **Hamming Loss:** it is the fraction of labels that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels.\n",
    "\\begin{equation}\n",
    "HL = \\frac{1}{N\\cdot L}\\sum_{i=1}^{N} \\sum_{j=1}^L \\mathcal{I}\\left\\{\\hat y^{(i)}_j \\neq y^{(i)}_j\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "$\\to$ for each instance compares each predicted label to actual label and counts the number of misclassified labels\n",
    "\n",
    "2. **0/1 loss (a.k.a. Exact Match Ratio):** It is the most strict metric, indicating the percentage of samples that have all their labels classified correctly.\n",
    "\\begin{equation}\n",
    "L_{0/1} =  \\frac{1}{N}\\sum_{i=1}^{N}  \\mathcal{I}\\left\\{\\hat{\\textbf{y}}^{(i)} \\neq \\textbf{y}^{(i)}\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "$\\to$ for each instance compares all the tpredicted label to the actual ones and counts the number of misclassified instances\n",
    "\n",
    "Note: There is a function in scikit-learn which implements subset accuracy, called ```accuracy_score```.\n",
    "3. **Accuracy:** : Accuracy for each instance is defined as the proportion of the predicted correct labels\n",
    "to the total number (predicted and actual) of labels for that instance. Overall accuracy is the average\n",
    "across all instances\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{1}{N}\\sum_{i=1}^N \\frac{|\\hat{\\textbf{y}}^{(i)} \\cap {\\textbf{y}}^{(i)} |}{|\\hat{\\textbf{y}}^{(i)} \\cup {\\textbf{y}}^{(i)} |}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "4. **Log Loss:** Sometimes we want to evaluate probabilities to encourage good ‘confidence’. Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. This gives us a more nuanced view into the performance of our model.\n",
    "\\begin{equation}\n",
    "\\text{Log-Loss} = - \\sum_{i=1}^N\\sum_{j=1}^L y_j^{(i)} \\log \\left(\\hat{y}_j^{(i)}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "5. **Ranking Loss:** to encourage good ranking; ranking loss evaluates the average proportion of label pairs that are incorrectly ordered for an instance.\n",
    "\n",
    "Other metrics used in the literature:\n",
    "- one error – if top ranked label is not in set of true labels\n",
    "- coverage – average “depth” to cover all true labels\n",
    "- precision\n",
    "- recall\n",
    "- macro-averaged F1 (ordinary averaging of a binary measure)\n",
    "- micro-averaged F1 (labels as different instances of a ‘global’ label)\n",
    "- precision vs. recall curves\n",
    "\n",
    "### Comments:\n",
    "- Hamming loss can in principal be minimized without taking label dependence into account.\n",
    "- For 0/1 loss label dependence must be taken into account.\n",
    "- Possible thresholding strategies:\n",
    "    1. Select a threshold $t$ from an internal validation test, e.g. $t\\in\\{0.1, 0.2, . . . , 0.9\\}$ $\\to$ slow!!!\n",
    "    2. Calibrate a threshold such that $LCard({\\textbf{y}}) ≈ LCard(\\hat{\\textbf{y}} )$\n",
    "       - e.g., training data has label cardinality of 1.7;\n",
    "       - set a threshold $t$ such that the label cardinality of the test data is as close as possible to 1.7\n",
    "\n",
    "    3. Calibrate $L$ thresholds such that each $LCard(y_j) ≈ LCard(\\hat{y}_j)$\n",
    "       - e.g., the frequency of label $y_j = 1$ is 0.3,\n",
    "       - set a threshold $t_j$ such that $h_j(\\textbf{x}) \\geq t_j \\leftrightarrow y_j = 1$ with frequency as close as possible to 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "## 1. Training \n",
    "### a) Problem Transformation Algorithms:\n",
    "Train the following models:\n",
    "1. Binary Relevance (does not model label dependencies)\n",
    "2. Label Powerset (models label dependencies)\n",
    "3. Classifier chains (models label dependencies)\n",
    "\n",
    "with:\n",
    "- NB\n",
    "- SVM\n",
    "- LogReg\n",
    "- NB-SVM\n",
    "\n",
    "### b) Algorithm Adaptation Methods:\n",
    "Train the following models:\n",
    "1. kNN\n",
    "2. Trees\n",
    "3. DL\n",
    "\n",
    "## 2. Evaluation:\n",
    "1. HL\n",
    "2. LogLoss\n",
    "3. Exact Match Ratio\n",
    "4. AUC score\n",
    "5. Micro/Macro F1-score\n",
    "6. Rank loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes and arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# sklearn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# sklearn metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss, hamming_loss, label_ranking_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# sklearn tools\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier # also implements multilabel clf just like MultiOutputClassifier (BR)\n",
    "from sklearn.multioutput import MultiOutputClassifier \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "# skmulilearn models/wrappers (for multi-label classification)\n",
    "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# utilities\n",
    "from utilities.constants import CLASSES\n",
    "from utilities.functions import load_objects\n",
    "\n",
    "# directories\n",
    "DATA_DIR = './data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "log_loss_scorer = make_scorer(log_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfv = load_objects(DATA_DIR + 'X_train_tfv')\n",
    "y = load_objects(DATA_DIR + 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 30000), (159571, 6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfv.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tfv, y, random_state=42, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (127656, 30000)\n",
      "X_val (31915, 30000)\n",
      "y_train (127656, 6)\n",
      "y_val (31915, 6)\n"
     ]
    }
   ],
   "source": [
    "for name, data in zip(['X_train', 'X_val', 'y_train', 'y_val'],[X_train, X_val, y_train, y_val]):\n",
    "    print(name, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR\n",
      "loss: 0.3490358005186171\n",
      "time: 0.26319313049316406 sec\n",
      "MultiOutput\n",
      "loss: 0.3490358005186171\n",
      "time: 0.2046949863433838 sec\n",
      "BinaryRelevane\n",
      "loss: 0.3490358005186171\n",
      "time: 369.0491449832916 sec\n"
     ]
    }
   ],
   "source": [
    "clfs = {'OvR': OneVsRestClassifier(MultinomialNB()),\n",
    "        'MultiOutput': MultiOutputClassifier(MultinomialNB()),\n",
    "        'BinaryRelevance': BinaryRelevance(MultinomialNB())\n",
    "       }\n",
    "for name, clf in clfs.items():\n",
    "\n",
    "    start_time = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    loss = log_loss(y_val, clf.predict(X_val))\n",
    "    end_time = time()\n",
    "    print(name)\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(\"time: {} sec\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn models are identical when it comes to **multi-label classification** and in turn, they are identical to BinaryRelevance. MultiOutput appears to be slighlty faster than OvR, while BR is substantially slower than the sklearn models. Henceforth, we will be using the MultiOutput wrapper to implement binary relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39714574605418335"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = MultiOutputClassifier(DummyClassifier(strategy='most_frequent'))\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "log_loss(y_val, dummy_clf.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_params = [\n",
    "    {\n",
    "        'estimator__alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2681982219918218"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_br = GridSearchCV(MultiOutputClassifier(MultinomialNB()), nb_params, scoring=log_loss_scorer, n_jobs=-1, cv=5)\n",
    "nb_br.fit(X_train_tfv, y)\n",
    "nb_br.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([('svd', TruncatedSVD(n_components=50)),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('lr', MultiOutputClassifier(LogisticRegression()))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = [\n",
    "    {\n",
    "        'svd__n_components': [50, 100, 150],\n",
    "        'lr__estimator__C': [10**n for n in range(-3,3) ],\n",
    "        'lr__estimator__penalty': ['l1', 'l2']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegBR\n",
      "log_loss: -1.1483635067766411\n",
      "time: 3706.1563770771027 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"LogRegBR\")\n",
    "print(\"log_loss: {}\".format(lr_br.best_score_))\n",
    "print(\"time: {} sec\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__estimator__C': 0.001,\n",
       " 'lr__estimator__penalty': 'l1',\n",
       " 'svd__n_components': 50}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_br.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = Pipeline([('svd', TruncatedSVD(n_components=10)),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('svm', MultiOutputClassifier(LinearSVC(random_state=12)))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = [\n",
    "    {\n",
    "        'svd__n_components': [20,50],\n",
    "        'svm__estimator__C': [10**n for n in range(-3,1)],\n",
    "        'svm__estimator__penalty': ['l2'],\n",
    "        'svm__estimator__tol': [10**n for n in range(-5,-2)]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "svm_br = GridSearchCV(svm_pipe, svm_params, scoring=log_loss_scorer, n_jobs=-1, cv=3)\n",
    "svm_br.fit(X_train_tfv, y)\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "log_loss: -1.407751627034678\n",
      "time: 1211.24240899086 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM\")\n",
    "print(\"log_loss: {}\".format(svm_br.best_score_))\n",
    "print(\"time: {} sec\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svd__n_components': 20,\n",
       " 'svm__estimator__C': 0.001,\n",
       " 'svm__estimator__penalty': 'l2',\n",
       " 'svm__estimator__tol': 1e-05}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_br.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5910059878314433"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val, svm_br.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NB-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "1. a Multinomial Naive Bayes classifier is a **linear (binary) classifier** ([proof](https://svivek.com/teaching/lectures/slides/naive-bayes/naive-bayes-linear.pdf)):\n",
    "\n",
    "\\begin{align}\n",
    "y^{(i)} &= \\text{sign}\\left(\\textbf{w}^T \\textbf{x}^{(i)}+b\\right)\\\\\n",
    "\\textbf{w} &= \\log \\left(\\frac{\\textbf{p}/||\\textbf{p}||_1}{\\textbf{q}/||\\textbf{q}||_1}\\right),\\quad  &&b= \\log\\left(\\frac{\\sum_i \\mathcal{I}\\left\\{ y^{(i)}=1 \\right\\} }{\\sum_i \\mathcal{I}\\left\\{ y^{(i)}=-1 \\right\\}}\\right)\\\\\n",
    "\\textbf{p} &= \\alpha + \\sum_{i} \\textbf{x}^{(i)}\\cdot \\mathcal{I}\\left\\{ y^{(i)}=1 \\right\\} ,\\quad &&\\textbf{q} = \\alpha + \\sum_{i} \\textbf{x}^{(i)} \\cdot \\mathcal{I}\\left\\{ y^{(i)}=-1 \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "2. SVM with NB features ([NBSVM](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)):\n",
    "\\begin{align}\n",
    "y^{(i)} &= \\text{sign}\\left({\\textbf{w}^\\prime}^T {\\textbf{x}^\\prime}^{(i)}+b\\right)\\\\\n",
    "{\\textbf{x}^\\prime}^{(i)} & = \\textbf{r} \\circ \\textbf{x}^{(i)} \\quad \\text{with} \\quad \\textbf{r} = \\log \\left(\\frac{\\textbf{p}/||\\textbf{p}||_1}{\\textbf{q}/||\\textbf{q}||_1}\\right)\\\\\n",
    "\\textbf{w}^\\prime & = (1-\\beta) \\,\\bar{w} + \\beta \\,\\textbf{w}\n",
    "\\end{align}\n",
    "\n",
    "Advanced Tips:\n",
    "1. **Binarization:**\n",
    " - Binarize features such that:  $\\qquad\\hat{\\textbf{x}}^{(i)} = \\mathcal{I}\\left\\{ \\textbf{x}^{(i)}>0\\right\\}$. \n",
    " - Calculate $\\textbf{p}, \\textbf{q}, \\textbf{r}$ using $\\hat{\\textbf{x}}^{(i)}$.\n",
    "\n",
    "\n",
    "2. **The Interpolation Parameter ($\\beta$):**\n",
    " - Best to set $\\beta$ in the range of $\\left[1/4, 1/2\\right]$. \n",
    " - To avoid further hyperparameter tuning, set $\\beta = 1$ s.t. $\\textbf{w}^\\prime = \\textbf{w}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, C=1.0, alpha=1.0, beta=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "\n",
    "    def _pr(self, X, y_i, y):\n",
    "        \n",
    "        p = X[y==y_i].sum(axis=0)\n",
    "        \n",
    "        return (self.alpha + p) / (self.alpha + (y==y_i).sum())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "\n",
    "        # Create NB features \n",
    "        self._r = sparse.csr_matrix(np.log(self._pr(X, 1, y) / self._pr(X, 0, y)))\n",
    "        X_nb = X.multiply(self._r)\n",
    "        \n",
    "        # Train a Logistic Regression\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(X_nb, y)\n",
    "        \n",
    "        # Define the scaled/shifted weight coefficients, intercept, classes\n",
    "        self.coef_ = (1 - self.beta) * np.mean(self._clf.coef_) + self.beta * self._clf.coef_\n",
    "        self.intercept_ = self._clf.intercept_\n",
    "        self.classes_ = self._clf.classes_\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        \n",
    "        n_features = self.coef_.shape[1]\n",
    "        if X.shape[1] != n_features:\n",
    "            raise ValueError(\"X has %d features per sample; expecting %d\"\n",
    "                             % (X.shape[1], n_features))\n",
    "\n",
    "        scores = safe_sparse_dot(X, self.coef_.T,\n",
    "                                 dense_output=True) + self.intercept_\n",
    "        return scores.ravel() if scores.shape[1] == 1 else scores\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_nb = X.multiply(self._r)\n",
    "        \n",
    "        scores = self.decision_function(X_nb)\n",
    "        \n",
    "        if len(scores.shape) == 1:\n",
    "            indices = (scores > 0).astype(np.int)\n",
    "        else:\n",
    "            indices = scores.argmax(axis=1)\n",
    "            \n",
    "        return self.classes_[indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbsvm_pipe = Pipeline([('svd', TruncatedSVD(n_components=200)),\n",
    "                       ('bin', Binarizer())\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bin = Binarizer().fit_transform(X_train)\n",
    "X_val_bin = Binarizer().fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta': 0.25, 'C': 0.1}\n",
      "-1.5510199858351847\n",
      "0.5275580051848201\n",
      "\n",
      "\n",
      "{'beta': 0.25, 'C': 1}\n",
      "-1.9510188547657694\n",
      "0.5475807806002264\n",
      "\n",
      "\n",
      "{'beta': 0.25, 'C': 5}\n",
      "-2.038474825754459\n",
      "0.5698241749687063\n",
      "\n",
      "\n",
      "{'beta': 0.25, 'C': 10}\n",
      "-2.0934426836777527\n",
      "0.5801653347225113\n",
      "\n",
      "\n",
      "{'beta': 0.5, 'C': 0.1}\n",
      "-1.9300572390883368\n",
      "0.5850152159434782\n",
      "\n",
      "\n",
      "{'beta': 0.5, 'C': 1}\n",
      "-2.005808811500884\n",
      "0.6131079997756285\n",
      "\n",
      "\n",
      "{'beta': 0.5, 'C': 5}\n",
      "-2.0026344640216296\n",
      "0.6313964645766436\n",
      "\n",
      "\n",
      "{'beta': 0.5, 'C': 10}\n",
      "-1.9818000417223052\n",
      "0.6383058998807447\n",
      "\n",
      "\n",
      "{'beta': 0.75, 'C': 0.1}\n",
      "-1.8165866551139214\n",
      "0.6480771896155139\n",
      "\n",
      "\n",
      "{'beta': 0.75, 'C': 1}\n",
      "-1.790321183894278\n",
      "0.6766445181476682\n",
      "\n",
      "\n",
      "{'beta': 0.75, 'C': 5}\n",
      "-1.787914138577026\n",
      "0.6935189544058692\n",
      "\n",
      "\n",
      "{'beta': 0.75, 'C': 10}\n",
      "-1.7729873772589277\n",
      "0.699735228047197\n",
      "\n",
      "\n",
      "{'beta': 1, 'C': 0.1}\n",
      "-1.5053393861506121\n",
      "0.7127344857056616\n",
      "\n",
      "\n",
      "{'beta': 1, 'C': 1}\n",
      "-1.5155525857834675\n",
      "0.7360703889668305\n",
      "\n",
      "\n",
      "{'beta': 1, 'C': 5}\n",
      "-1.5499886943702532\n",
      "0.7457686457044129\n",
      "\n",
      "\n",
      "{'beta': 1, 'C': 10}\n",
      "-1.5751757904004289\n",
      "0.7486120362473283\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = 100\n",
    "for beta in [0.25, 0.5, 0.75, 1]:\n",
    "    for C in [0.1, 1, 5, 10]:\n",
    "        nbsvm_clf = MultiOutputClassifier(NbSvmClassifier(C=C, beta=beta, dual=True, n_jobs=-1)).fit(X_train_bin, y_train)\n",
    "        y_preds = nbsvm_clf.predict(X_val_bin)\n",
    "        loss = -log_loss(y_val, y_preds)\n",
    "        print({'beta': beta, 'C': C})\n",
    "        print(loss)\n",
    "        print(roc_auc_score(y_val, y_preds))\n",
    "        print('\\n')\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_clf = nbsvm_clf\n",
    "            best_params = {'beta': beta, 'C': C}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

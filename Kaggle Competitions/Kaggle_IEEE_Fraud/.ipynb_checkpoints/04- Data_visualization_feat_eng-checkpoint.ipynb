{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.fraud_pre_proc import *\n",
    "from utils.fraud_feat_engineering import *\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning knobs \n",
    "\n",
    "### 1. Data Cleaning\n",
    "A. Number of low NaN-rate features: nans_rate_cut_off (parameter) <br>\n",
    "B. \n",
    "- Numerical_categorical split: min_categories (parameter)\n",
    "- Methods to fill NaNs: Vasilis or Papes  <br>\n",
    "\n",
    "### 2. Feature Engineering \n",
    "A. Which Datetime Feats to add: select manually <br>\n",
    "B1. Which Card-Address Interaction Feats to add: : select manually <br>\n",
    "B2. Which Card-Address-Datetime Interaction Feats to add:<br>\n",
    "$\\quad$ i) period_feats (list)<br>\n",
    "$\\quad$ ii) card_addr_feats (list)<br>\n",
    "C. Which Aggregated TransAmt Feats to add: select manually <br>\n",
    "D. Which Frequency Feats to add: select manually <br>\n",
    "\n",
    "### 3. Preprocessing and Feature Selection \n",
    "Numerical_categorical split: min_categories (parameter)<br>\n",
    "A. Number of highly correlated features: corr_cut_off (parameter) <br>\n",
    "B. Method of treating categorical feats: how = {'dummies','label_enc'} <br>\n",
    "\n",
    "### 4. Stratified Split \n",
    "- Stratified split parameters: frac, n_splits<br>\n",
    "- (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trans data\n",
    "df_all = pd.read_csv('./Data/df_trans_ids_imp.csv')\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickleObjects import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/'\n",
    "\n",
    "cat_cols = loadObjects(path+'cat_cols_all')\n",
    "num_cols = loadObjects(path+'num_cols_all')\n",
    "cols = loadObjects(path+'cols_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Datetime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_feats=addDatetimeFeats(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #B.1 Add Interaction Features by ADDING the values of card_ and addr_ columns\n",
    "# card_addr_interactions = addCardAddressInteractionFeats(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all[card_addr_interactions].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# card_addr_feats = card_addr_interactions + ['card1','card2','card3','card5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B.2 Add interaction features by ADDING the values of card_addr_feats and period_feats\n",
    "#   and computing value frequencies\n",
    "# new_feats = addDatetimeInteractionFeats(df_all, cols=card_addr_feats, period_cols=period_feats);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Aggregated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = ['card1','card2','card3','card5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregated features by grouping-by card_addr_feats and computing the mean & STD of 'TransactionAmt'\n",
    "agg_feats = addAggTransAmtFeats(df_all,cols=cards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Indicator/Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_cols = cards + ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "        'D1','D2','D3','D4','D5','D6','D7','D8',\n",
    "        'addr1','addr2',\n",
    "        'dist1','dist2',\n",
    "        'P_emaildomain', 'R_emaildomain',\n",
    "        'DeviceInfo','DeviceType',\n",
    "        'id_30','id_33']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add indicator features by computing the value frequencies of try_cols\n",
    "freq_feats = addFrequencyFeats(df_all,cols=try_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_null_cols(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = period_feats + agg_feats + freq_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing and Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = pd.read_csv('./Data/train_transaction.csv',usecols = ['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all.iloc[:len(df_fraud),:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['isFraud'] = df_fraud['isFraud'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = TableDescriptor(df_train,'All_data','isFraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Filter Features by Correlation to target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert cat cols to cat vars\n",
    "numerical_vars = [var for var in all_vars.variables if var.name in num_cols+new_cols]\n",
    "#select high-correlated cat vars\n",
    "num_vars = getCorrelatedFeatures(numerical_vars,corr_cut_off=0.005)\n",
    "#list of high-correlated cat cols\n",
    "new_num_cols = [var.name for var in num_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert cat cols to cat vars\n",
    "categorical_vars = [var for var in all_vars.variables if var.name in cat_cols]\n",
    "#select high-correlated cat vars\n",
    "cat_vars = getCorrelatedFeatures(categorical_vars,corr_cut_off=0.1)\n",
    "#list of high-correlated cat cols\n",
    "new_cat_cols = [var.name for var in cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Convert categorical data to Dummies or Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = new_cat_cols + new_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_cols.remove('TransactionDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = to_categorical(df=df_all[all_cols],cat_cols=new_cat_cols,how='dummies')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_null_cols(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature scalingmodel.best_performance\n",
    "num_cols.remove('TransactionDT')\n",
    "X_train = df_all[new_num_cols].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "\n",
    "\n",
    "PCAT = PCATransformer(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['pca_error_2'] = PCAT.rec_error(X_train_scaled).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stratified Split training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dummy = df_all.iloc[:len(df_fraud),:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [col for col in df_train_dummy.columns.tolist() if col not in ['TransactionID','isFraud']]\n",
    "y_col = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y of df_train\n",
    "\n",
    "X, y = df_train_dummy.loc[:,x_cols].values, df_train.loc[:,y_col].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = getStratifiedTrainTestSplit(X,y,frac=0.2,n_splits=1,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df's shapes\n",
    "\n",
    "for i in [X_train, X_test, y_train, y_test]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_all.loc[len(df_fraud):,x_cols].copy()\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save analyzed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickleObjects import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/'\n",
    "\n",
    "dumpObjects(X_train,path+'X_train')\n",
    "dumpObjects(y_train,path+'y_train')\n",
    "dumpObjects(X_test,path+'X_test')\n",
    "dumpObjects(y_test,path+'y_test')\n",
    "dumpObjects(df_test.values,path+'X_test_comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
